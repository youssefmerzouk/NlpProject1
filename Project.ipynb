{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "data = pd.read_csv(\"sentences.csv\")\n",
    "data = data.dropna()\n",
    "data['english'] = data['eng']\n",
    "english_sentences = data['english'].tolist()\n",
    "darija_sentences = data['darija'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset into train, validation, and test sets\n",
    "train_eng, temp_eng, train_dar, temp_dar = train_test_split(english_sentences, darija_sentences, test_size=0.2)\n",
    "val_eng, test_eng, val_dar, test_dar = train_test_split(temp_eng, temp_dar, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src shape: torch.Size([32, 50])\n",
      "trg shape: torch.Size([32, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_24272\\3366960100.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {key: torch.tensor(val[idx]) for key, val in self.encodings.items()},\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_24272\\3366960100.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(self.labels[\"input_ids\"][idx])\n"
     ]
    }
   ],
   "source": [
    "#initialize tokenizer for both English and Darija (using a multilingual model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-ar\")\n",
    "\n",
    "#tokenize the sentences\n",
    "train_encodings = tokenizer(train_eng, padding=True, truncation=True, return_tensors=\"pt\", max_length=50)\n",
    "train_labels = tokenizer(train_dar, padding=True, truncation=True, return_tensors=\"pt\", max_length=50)\n",
    "\n",
    "#dataset class\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            {key: torch.tensor(val[idx]) for key, val in self.encodings.items()},\n",
    "            torch.tensor(self.labels[\"input_ids\"][idx])\n",
    "        )\n",
    "\n",
    "#create the Dataset and DataLoader\n",
    "train_dataset = TranslationDataset(train_encodings, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "#check the first batch to confirm it's working\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    src, trg = batch\n",
    "    print(f\"src shape: {src['input_ids'].shape}\")\n",
    "    print(f\"trg shape: {trg.shape}\")\n",
    "    break  # Print just the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_24272\\3366960100.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  {key: torch.tensor(val[idx]) for key, val in self.encodings.items()},\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_24272\\3366960100.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(self.labels[\"input_ids\"][idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 5.595041040342803\n",
      "Epoch 2, Loss: 4.91170379883817\n",
      "Epoch 3, Loss: 4.692343031724793\n",
      "Epoch 4, Loss: 4.505145769507908\n",
      "Epoch 5, Loss: 4.320055384620978\n",
      "Epoch 6, Loss: 4.137181234210263\n",
      "Epoch 7, Loss: 3.9512719517591233\n",
      "Epoch 8, Loss: 3.759408745272406\n",
      "Epoch 9, Loss: 3.5723355309716585\n",
      "Epoch 10, Loss: 3.393462480796168\n"
     ]
    }
   ],
   "source": [
    "#define the LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, emb_dim, hidden_dim, dropout_rate=0.3):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        outputs = self.dropout(outputs)\n",
    "        predictions = self.fc(outputs)\n",
    "        return predictions\n",
    "\n",
    "#initialize the model with the correct vocab size\n",
    "vocab_size = tokenizer.vocab_size\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMModel(input_dim=vocab_size, output_dim=vocab_size, emb_dim=256, hidden_dim=512).to(device)\n",
    "\n",
    "#adamW optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)  # Ignore padding token during loss calculation\n",
    "\n",
    "#learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)\n",
    "\n",
    "#gradient clipping\n",
    "clip_value = 1.0\n",
    "\n",
    "#tensorBoard for monitoring\n",
    "writer = SummaryWriter()\n",
    "\n",
    "#training loop\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        #unpack the batch\n",
    "        src, trg = batch\n",
    "        \n",
    "        #move tensors to the device\n",
    "        src = src[\"input_ids\"].to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #forward pass\n",
    "        output = model(src)\n",
    "        \n",
    "        #flatten the output and target for loss calculation\n",
    "        output = output.view(-1, output.shape[-1])  # Flatten to [batch_size * seq_len, vocab_size]\n",
    "        trg = trg.view(-1)  # Flatten target to [batch_size * seq_len]\n",
    "\n",
    "        #mask padding tokens\n",
    "        mask = trg != tokenizer.pad_token_id  # Mask padding tokens from target\n",
    "        output = output[mask]  # Apply the mask to output\n",
    "        trg = trg[mask]  # Apply the mask to target\n",
    "        \n",
    "        #compute loss\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()  # Backpropagation\n",
    "        \n",
    "        #gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "        \n",
    "        optimizer.step()  # Update model parameters\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    #adjust the learning rate using scheduler\n",
    "    scheduler.step(epoch_loss)\n",
    "    \n",
    "    #log loss to TensorBoard\n",
    "    writer.add_scalar('Loss/train', epoch_loss / len(train_loader), epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss / len(train_loader)}\")\n",
    "\n",
    "#close TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive Translation Test (Type 'exit' to quit)\n",
      "English: Thank you for your help.\n",
      "Translated (Darija): chokran 3la la\n",
      "--------------------------------------------------\n",
      "English: Thank you for your help.\n",
      "Translated (Darija): chokran 3la lla\n",
      "--------------------------------------------------\n",
      "English: Thank you for your help.\n",
      "Translated (Darija): chokran 3la 3 l\n",
      "--------------------------------------------------\n",
      "English: Thank you for your help.\n",
      "Translated (Darija): chokran 3lik la\n",
      "--------------------------------------------------\n",
      "English: They're hiding something, I'm sure!\n",
      "Translated (Darija): rah kaybby ha ,,,a7\n",
      "--------------------------------------------------\n",
      "English: They're hiding something, I'm sure!\n",
      "Translated (Darija): gha kaybby ha7,,,,a\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "#initialize tokenizer and model (assuming they are already defined and trained)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-ar\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#function to generate translations\n",
    "def generate_translation(model, tokenizer, sentence, device):\n",
    "    #tokenize the input sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=50).to(device)\n",
    "    \n",
    "    #generate translation using the model\n",
    "    with torch.no_grad():\n",
    "        output = model(inputs[\"input_ids\"])  # Get model output\n",
    "        pred_tokens = torch.argmax(output, dim=-1)  # Get the index of the highest probability token for each position\n",
    "        \n",
    "    #decode the predicted tokens to get the translated sentence\n",
    "    pred_sentence = tokenizer.decode(pred_tokens[0], skip_special_tokens=True)\n",
    "    return pred_sentence\n",
    "\n",
    "# Start interactive loop\n",
    "print(\"Interactive Translation Test (Type 'exit' to quit)\")\n",
    "while True:\n",
    "    #ask for user input\n",
    "    input_sentence = input(\"Enter an English sentence: \")\n",
    "    \n",
    "    #exit condition\n",
    "    if input_sentence.lower() == \"exit\":\n",
    "        print(\"Exiting interactive mode.\")\n",
    "        break\n",
    "    \n",
    "    #generate the translation\n",
    "    translated_sentence = generate_translation(model, tokenizer, input_sentence, device)\n",
    "    \n",
    "    #output the result\n",
    "    print(f\"English: {input_sentence}\")\n",
    "    print(f\"Translated (Darija): {translated_sentence}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#load your data (example: sentences.csv)\n",
    "data = pd.read_csv('sentences.csv')\n",
    "\n",
    "#ensure no null values in 'eng' or 'darija' columns\n",
    "data = data.dropna(subset=['eng', 'darija'])\n",
    "\n",
    "#split the data into train and test sets\n",
    "train, test = train_test_split(data, test_size=0.2)\n",
    "\n",
    "#convert to Hugging Face dataset format\n",
    "dataset = Dataset.from_pandas(train[['eng', 'darija']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde2ebbae98f425eb81e4f7c3065ff04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10194 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:4114: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianTokenizer\n",
    "\n",
    "#initialize the MarianMT tokenizer\n",
    "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-ar\")\n",
    "\n",
    "#define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(examples['eng'], padding=\"max_length\", truncation=True, max_length=50)\n",
    "    \n",
    "    #use target tokenizer for 'darija' column\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['darija'], padding=\"max_length\", truncation=True, max_length=50)\n",
    "    \n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "#apply the tokenization to the entire dataset\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "#convert to PyTorch format\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#create DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.154708115471568\n",
      "Epoch 2, Loss: 0.7374351003607239\n",
      "Epoch 3, Loss: 0.5758813704629677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62801]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('fine_tuned_english_darija\\\\tokenizer_config.json',\n",
       " 'fine_tuned_english_darija\\\\special_tokens_map.json',\n",
       " 'fine_tuned_english_darija\\\\vocab.json',\n",
       " 'fine_tuned_english_darija\\\\source.spm',\n",
       " 'fine_tuned_english_darija\\\\target.spm',\n",
       " 'fine_tuned_english_darija\\\\added_tokens.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "import torch\n",
    "\n",
    "#initialize the AdamW optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "#training loop\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:  #iterate over batches in the DataLoader\n",
    "        input_ids = batch['input_ids'].to(device)  #move inputs to device (GPU/CPU)\n",
    "        attention_mask = batch['attention_mask'].to(device)  #move attention mask to device\n",
    "        labels = batch['labels'].to(device)  #move labels to device\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #forward pass: get model outputs (logits)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss  #the loss is returned in the outputs\n",
    "\n",
    "        loss.backward()  #backpropagation: calculate gradients\n",
    "        optimizer.step() #update model weights\n",
    "\n",
    "        epoch_loss += loss.item()  #add the loss to epoch loss\n",
    "\n",
    "    #print average loss for this epoch\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(train_loader)}\")\n",
    "\n",
    "#save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(\"fine_tuned_english_darija\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_english_darija\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: How are you?\n",
      "Translated (Darija): kidayr?\n",
      "--------------------------------------------------\n",
      "English: Thank you for your help.\n",
      "Translated (Darija): chokran 3la lmosa3ada dyalk.\n",
      "--------------------------------------------------\n",
      "English: Are there any other managers on her level\n",
      "Translated (Darija): wach kayn chi lmodirat khrin f lmosta9 dyalha\n",
      "--------------------------------------------------\n",
      "English: I love translation.\n",
      "Translated (Darija): kat3jbni trjem.\n",
      "--------------------------------------------------\n",
      "English: What is your name?\n",
      "Translated (Darija): chnou smitk?\n",
      "--------------------------------------------------\n",
      "English: This is a you?\n",
      "Translated (Darija): hada nta?\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "\n",
    "#load the fine-tuned model and tokenizer\n",
    "model_name = \"fine_tuned_english_darija\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "#function to generate translations\n",
    "def generate_translation(model, tokenizer, sentence, device):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=50).to(device)\n",
    "    \n",
    "    #generate translation using the model\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(inputs[\"input_ids\"])  # Get translated output\n",
    "    \n",
    "    #gecode the translated tokens into a sentence\n",
    "    translated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return translated_sentence\n",
    "\n",
    "#example list of test sentences\n",
    "test_sentences = [\n",
    "    \"How are you?\",\n",
    "    \"Thank you for your help.\",\n",
    "    \"Are there any other managers on her level\",\n",
    "    \"I love translation.\",\n",
    "    \"What is your name?\",\n",
    "    \"This is a you?\"\n",
    "]\n",
    "\n",
    "#translate each sentence\n",
    "for sentence in test_sentences:\n",
    "    translated_sentence = generate_translation(model, tokenizer, sentence, device)\n",
    "    print(f\"English: {sentence}\")\n",
    "    print(f\"Translated (Darija): {translated_sentence}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: How are you?\n",
      "Reference (Darija): Kif dayr?\n",
      "Model Translation (Darija): kidayr?\n",
      "--------------------------------------------------\n",
      "English: Thank you for your help.\n",
      "Reference (Darija): Choukran 3la musa3datk.\n",
      "Model Translation (Darija): chokran 3la lmosa3ada dyalk.\n",
      "--------------------------------------------------\n",
      "English: I love coding.\n",
      "Reference (Darija): Kan7eb coding.\n",
      "Model Translation (Darija): kat3jbni ttrejm.\n",
      "--------------------------------------------------\n",
      "English: What is your name?\n",
      "Reference (Darija): Shno smitik?\n",
      "Model Translation (Darija): chnou smitk?\n",
      "--------------------------------------------------\n",
      "English: This is a test sentence.\n",
      "Reference (Darija): Hadi jumla tajaribiya.\n",
      "Model Translation (Darija): hadi joj lmti7tar.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "\n",
    "#load the fine-tuned model and tokenizer\n",
    "model_name = \"fine_tuned_english_darija\"  # Your model's path\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "#load the test dataset\n",
    "test_data = pd.read_csv(\"darija.csv\")\n",
    "\n",
    "#function to generate translations\n",
    "def generate_translation(model, tokenizer, sentence, device):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=50).to(device)\n",
    "    \n",
    "    #generate translation using the model\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(inputs[\"input_ids\"])  # Get translated output\n",
    "    \n",
    "    #decode the translated tokens into a sentence\n",
    "    translated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return translated_sentence\n",
    "\n",
    "#test translations on sentences from the CSV\n",
    "for index, row in test_data.iterrows():\n",
    "    sentence = row['english']\n",
    "    true_translation = row['darija']\n",
    "    \n",
    "    #generate the translation using the model\n",
    "    translated_sentence = generate_translation(model, tokenizer, sentence, device)\n",
    "    \n",
    "    print(f\"English: {sentence}\")\n",
    "    print(f\"Reference (Darija): {true_translation}\")\n",
    "    print(f\"Model Translation (Darija): {translated_sentence}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
